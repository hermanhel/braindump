+++
title = "Linear Algebra"
author = ["System Administrator"]
draft = false
+++

Linear Algebra is set of algebra `formulated rules`, `formulated methods` and `found facts`


## Resource {#resource}

1.  This note is originally a reading note on [math4ml](http://gwthomas.github.io/docs/math4ml.pdf%5D%5Bhttp://gwthomas.github.io/docs/math4ml.pdf).
2.  I'm adding my own insights afterwards.


## Spaces {#spaces}


### Vector space {#vector-space}

set of vectors defining:

-   `add` operation: v1 + v2
-   `scale` operation: n \* v1


#### propositions {#propositions}

-   0: x + 0 = x
-   -x: for each x, exist -x + x = 0
-   1: 1x = x
-   commutativity: x+y=y+x
-   Associativity: (x+y)+z=x+(y+z),(ab)x=a(bx)
-   Distributivity: a(x + y) = ax + ay


#### linear combination {#linear-combination}

-   set of vectors: \\(V=\\{v\_1,v\_2...\\}\\)
-   set of real numbers: \\(R = \\{a\_1,a\_2...\\}\\)
-   `linear combination`: \\(a\_1v\_1 + a\_2v\_2+...+a\_nv\_n\\)


#### linear independent {#linear-independent}

only combination of \\(a\_1v\_1+a\_2v\_2+a\_3v\_3+...=0\\) is \\(a\_1 = a\_2 = ... = 0\\)


#### span {#span}

\\(span(V)\\) is set that: for every \\(v \in span(V)\\), \\(v\\) can be written as a [linear combination](#linear-combination)
of \\(V\\).


#### basis {#basis}

basis is a set of vector \\(V\\) such that:

-   \\(V\\) is linear independent
-   with respect to \\(span(V)\\)

for every set of linear independent vectors, they are a basis for its span.


#### finite-dimensional <span class="tag"><span class="confusing">confusing</span></span> {#finite-dimensional}

vector space \\(V\\) that is spanned by a finite number of vectors.

-   Infinite-dimensional vector space: spanned by infinite number of vectors
-   dimension: \\(dim(V)\\) of \\(span(V)\\) is the number of vectors in \\(V\\)


### Euclidean space {#euclidean-space}

\\(\mathbb{R}^n\\). [Vector space](#vector-space) [span](#span)ned by a n-tuple [linear independent](#linear-independent) [basis](#basis).

-   used to represent physical space.


#### subspace {#subspace}

\\(S\\) is part of \\(V\\)

<!--list-separator-->

-  propositions

    -   0: \\(0 \in S\\)
    -   +: \\(S\\) is closed under addition
    -   \*: \\(S\\) is closed under scale.
    -   self: \\(V\\) is a subspace of \\(V\\), 0 is a subspace of 0
    -   space+space: \\(U \in V + W \in V = S \in V\\)
    -   dim of space+space: \\(dim(U + W) = dim(U)+dim(W)-dim(U \cap W)\\)

<!--list-separator-->

-  direct sum

    \\(U \oplus W\\) when \\(U \cup W =\\{0\\}\\).

    <!--list-separator-->

    -  propositions

        -   unique linear combination: each \\(v\\) in \\(U \oplus W\\) can be uniquely written as \\(u + w\\)
        -   \\(dim(U + W) = dim(U)+dim(W)\\)


### Metric space {#metric-space}


### Normed Space {#normed-space}


### Inner product space {#inner-product-space}


## Linear Map {#linear-map}

a function \\(T:V \rightarrow W\\)


### satisfies: {#satisfies}

-   T(x+y)=T(x)+T(y)
-   T(ax)=aT(x)

identical sturcutr to Vector space


### Linear Operator {#linear-operator}

a [Linear Map](#linear-map) from \\(V\\) to \\(V\\)


### homomorphism <span class="tag"><span class="confusing">confusing</span></span> {#homomorphism}

a map is a homomorphism if:

-   input and output is the same type(both are Vector Space)
-   the structure is preserved


#### is a 3d to 2d map homomorphism? {#is-a-3d-to-2d-map-homomorphism}


#### ismorphism {#ismorphism}

invertible [homomorphism](#homomorphism)

-   \\(V\\) and \\(W\\) are `ismorphic` when \\(\exists\\) homomorphism between \\(V\\) and \\(W\\): \\(V \cong W\\)
-   ismorphic vecter spaces are `the same` in `algebraic structure` (both [x1,x2,x3,x4])

<!--list-separator-->

-  proposition

    -   dimension: finite-dimensional vector space of the same dimension are ismorphic
    -   to Eucliead: any n-dimensional vector space is ismorphic to \\(\mathbb{R}^n\\)
        -   proof:
            at least exist a map that map n bases of \\(V\\) to n bases of \\(W\\).

            \begin{aligned}
            \varphi: V & \rightarrow W \\\\
            \alpha\_{1} \mathbf{v}\_{1}+\cdots+\alpha\_{n} \mathbf{v}\_{n} & \mapsto \alpha\_{1} \mathbf{w}\_{1}+\cdots+\alpha\_{n} \mathbf{w}\_{n}
            \end{aligned}


####  {#d41d8c}


### Matrix of Linear Map {#matrix-of-linear-map}

for \\(T:V \rightarrow W\\) where \\(V\\) have basis \\(v\_1,v\_2,...,v\_n\\), \\(W\\) have basis \\(w\_1,w\_2,...,w\_m\\).


#### formulated notation {#formulated-notation}

for \\(Tx = Ax = v\\), \\(A\\) is matrix representing [Linear Map](#linear-map) T

-   A consist of:
    -   columns: base vectors in column space \\(V\\).
-   x is vector in row space \\(V\\), that will be mapped into \\(W\\) by \\(A\\).


#### Transpose {#transpose}

flip the matrix by diagnoal, row-&gt;column, column-&gt;row

<!--list-separator-->

-  operations

    -   double: \\((A^T)^T = A\\)
    -   \\((A+B)^T=A^T+B^T\\)
    -   \\((aA)^T=aA^T\\)
    -   \\((AB)^T=B^TA^T\\)


#### <span class="org-todo todo TODO">TODO</span> Column space and row space {#column-space-and-row-space}

difference between left product and right product, how do they(column space and row space) connect.


#### domain {#domain}

\\(V\\)


#### codomain {#codomain}

\\(W\\)


#### Nullspace {#nullspace}

\\(null(T)=\\{v \in V | Tv=0\\}\\)

-   the space being mapped into 0
-   subspace of V(domain)


#### Range {#range}

\\(range(T)=\\{w\in W|\exists v \in V\\) such that \\(Tv=w\\}\\)

-   parts of \\(W\\) that is filled by \\(TV\\)
-   subspace of W(codomain)


#### columnspace {#columnspace}

span of \\(A\\)'s columns.

-   exactly the range of \\(A\\). so denoted by \\(range(A)\\)

<!--list-separator-->

- <span class="org-todo todo TODO">TODO</span>  Formulated proof of columnspace=range(A)


#### rowspace {#rowspace}

span of \\(A\\)'s rows.

-   exactly the range of \\(A^T\\). so denoted by \\(range(A^T\\))


#### rank {#rank}

the [dimension](#dimension) of [columnspace](#columnspace) and [rowspace](#rowspace) is the same. And that's the rank of \\(A\\).


### Metric Space {#metric-space}

a set with a difined [metric](#metric) is a metric space

generalization of `distant` from [Euclidean space](#euclidean-space)


#### metric {#metric}

distant function.

a metric \\(d\\) on set \\(S\\) is a function \\(d: S \times S \rightarrow \mathbb{R}\\)
the \\(S \times S\\) means \\(d\\) take 2 args from set \\(S\\)

motivation: to define limit to enable calculus.

<!--list-separator-->

-  propositions

    -   \\(d(x,y) \ge 0\\)
    -   \\(d(x,y)=d(y,x)\\)
    -   \\(d(x,z) \ge d(x,y)+d(y,z)\\)

<!--list-separator-->

-  example in sequence

    converge: d(x_n,x)&lt;e, where d(x_n,x) is defined as d(x,y)=|x - y|


### Normed space {#normed-space}

a [Vector space](#vector-space) with [norm](#norm) defined.

generalization of `length` from [Euclidean space](#euclidean-space)


#### norm {#norm}

a function ∥·∥:V →R.

<!--list-separator-->

-  propositions

    -   \\(||x|| \ge 0\\)
    -   \\(||ax|| = |a|||x||\\)
    -   \\(||x+y|| \le ||x||+||y||\\)
    -   a [Normed space](#normed-space) is a [Metric space](#metric-space)
    -   in [finite-dimensional](#finite-dimensional) space, all norms converge the same time.
        -   proof:

<!--list-separator-->

-  important norms

    \begin{aligned}
    \\|\mathbf{x}\\|\_{1} &=\sum\_{i=1}^{n}\left|x\_{i}\right| \\\\
    \\|\mathbf{x}\\|\_{2} &=\sqrt{\sum\_{i=1}^{n} x\_{i}^{2}} \\\\
    \\|\mathbf{x}\\|\_{p} &=\left(\sum\_{i=1}^{n}\left|x\_{i}\right|^{p}\right)^{\frac{1}{p}} \quad(p \geq 1) \\\\
    \\|\mathbf{x}\\|\_{\infty} &=\max \_{1 \leq i \leq n}\left|x\_{i}\right|
    \end{aligned}

    1: sum of cordinates
    2: Pythagorean distance


### Inner product space {#inner-product-space}

a real [Vector space](#vector-space) that defines [inner product](#inner-product)


#### inner product {#inner-product}

function \\(\langle\cdot, \cdot\rangle: V \times V \rightarrow \mathbb{R}\\)

<!--list-separator-->

-  propositions

    -   greater than 0
    -   linear in the first slot.
        -   why?
    -   no direction
    -   inner produt induce a norm \\(||x||=\sqrt{\langle x,x \rangle}\\), so an [Inner product space](#inner-product-space) is also a [Normed space](#normed-space)

<!--list-separator-->

-  standard inner product on \\(\mathbb{R}^{n}\\)

    \\(\langle\mathbf{x}, \mathbf{y}\rangle=\sum\_{i=1}^{n} x\_{i} y\_{i}=\mathbf{x}^{\top} \mathbf{y}\\)


#### orthogonal {#orthogonal}

vectors x and y are orthogonal if \\(\langle\cdot, \cdot\rangle = 0\\)

<!--list-separator-->

-  orthonormal

    vectors x and y are orthonormal if:

    -   \\(\langle x, y \rangle = 0\\)
    -   \\(||x||=||y||=1\\)

<!--list-separator-->

-  Pythagorean Theorem

<!--list-separator-->

-  Cauchy-Schwarz inequality

    comes in handy sometimes in proving bounds
    \\(| \langle x,y \rangle| \le ||x|| ||y||\\)

    -   Equal when:
        -   x = ay
            when they are [linear independent](#linear-independent)

<!--list-separator-->

-  orthogonal complement

    for \\(S \sube V\\) where \\(V\\) is an [Inner product space](#inner-product-space):
    \\(S^{\perp}\\), the `orthogonal complement` of S, is
    \\(S^{\perp}=\\{\mathbf{v} \in V \mid \mathbf{v} \perp \mathbf{s}\\) for all \\(\mathbf{s} \in S\\}\\)
    i.e. all other vectors in V that is orthogonal to all vectors in S

    <!--list-separator-->

    -  propositions

        <!--list-separator-->

        -  when S is a finite dimensional subspace of V

            -   \\(v = v\_S+v\_{\perp}\\) for every \\(v \in V\\)
                -   when V is a inner product space

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  proof

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  proof of uniqueness

        <!--list-separator-->

        -  orthogonal projection

            \begin{aligned} P<sub>S</sub>: &amp; V &rarr; S \\\\ \mathbf{v} &amp; \mapsto \mathbf{v}<sub>S</sub> \end{aligned}The mapping from V to S. Project to the (plane) space S

            <!--list-separator-->

            -  propoties

                1.  content: \\(P\_Sv=\langle v,u\_1 \rangle u\_1 + ... + \langle v,u\_m \rangle u\_m\\)
                    u_1 are [orthonormal](#orthonormal) basis of _S_
                2.  triangle \\(v-P\_sv \perp S\\)
                3.  \\(P\_S\\) is a linear map
                4.  \\(P\_Ss=s\\)
                5.  \\(range(P\_S)=S\\), \\(null(P\_S)=S^{\perp}\\)
                6.  \\(P\_S^2=P\_S\\)
                7.  \\(||P\_Sv|| \le ||v||\\)
                8.  shortest path:
                    \\(\left\\|\mathbf{v}-P\_{S} \mathbf{v}\right\\| \leq\\|\mathbf{v}-\mathbf{s}\\|\\)
                    with equality if and only if \\(\mathbf{s}=P\_{S} \mathbf{v}\\). That is,
                    \\(P\_{S} \mathbf{v}=\underset{\mathbf{s} \in S}{\arg \min }\\|\mathbf{v}-\mathbf{s}\\|\\)
                9.  \\(P\_S=\sum^{m}\_{i=1}u\_iu\_i^T=UU^T\\), where \\(U\\) have \\(u\_1,...,u\_m\\) as it's columns

                <!--list-separator-->

                - <span class="org-todo todo TODO">TODO</span>  Proof of the propoty 8

                    x-devonthink-item://717C4CDF-09AB-41FA-95C1-BBB9155432C6?page=12

        <!--list-separator-->

        -  projection

            linear map \\(P\\) that:

            -   \\(P^2 = P\\)

    <!--list-separator-->

    -  S don't have to be a subspace of V? <span class="tag"><span class="confusing">confusing</span></span>

    <!--list-separator-->

    -  argmin? <span class="tag"><span class="confusing">confusing</span></span>


## Eigenthings {#eigenthings}

for any \\(Ax=\lambda x\\), i.e. after [Linear Map](#linear-map) \\(x\\) only scaled on its direction:

-   \\(\lambda\\) is `eigenvalue`
-   \\(x\\) is `eigenvector`
-   0 is excluded in vector, but not eigenvalue.


### propositions: for \\(Ax=\lambda x\\) {#propositions-for--ax-lambda-x}

1.  \\(x\\) is an `eigenvector` of \\(A+aI\\) with `eigenvalue` \\(\lambda + a\\)
2.  if A is [invertible](#invertible), x is an `eigenvector` of \\(A^{-1}\\) with =eigenvalue \\(\lambda^{-1}\\)
3.  \\(A^kx=\lambda^kx\\)


#### <span class="org-todo todo TODO">TODO</span> proof of 1,2,3 {#proof-of-1-2-3}


## Trace {#trace}

Trace of a `square matrix` is the sum of its diagonal entries
\\(tr(A)=\sum\_{i=1}^n A\_{ii}\\)


### properties {#properties}

1.  \\(tr(A+B)=tr(A)+tr(B)\\)
2.  \\(tr(\alpha A)=\alpha tr(b)\\)
3.  \\(tr(A^T)=tr(A)\\)
4.  \\(tr(ABCD)=tr(BCDA)=tr(CDAB)=tr(DABC)\\)
5.  trace of A equals to the sum of its `eigenvalues`


## Determinant {#determinant}


### properties {#properties}

1.  det(I)= 1
2.  \\((detA^T)=det(A)\\)
3.  det(A)det(B)=det(AB)
4.  \\((detA^{-1})=det(A)^{-1}\\)
5.  \\(det(\alpha A)=\alpha^n det(A)\\)
6.  determinant of A is the product of its `eigenvalues`


## orthogonal matrix {#orthogonal-matrix}

an orthogonal matrix have its colulmns [orthonormal](#orthonormal) pairwise(every pair)


### propositions {#propositions}

1.  \\(Q^TQ=QQ^T=I\\), \\(Q^T=Q^{-1}\\)
2.  [inner product](#inner-product) is reserved the same after both are mapped. (rotate/reflax)
3.  [norm](#norm) is reserved the same after both are mapped. (rotate/reflax)


## symmetric matrix {#symmetric-matrix}

symmetric matrix \\(A=A^T\\)


### propositions {#propositions}


#### Spectral Theorem {#spectral-theorem}

a square matrix A is a [symmetric matrix](#symmetric-matrix) -&gt; \\(\exists\\) an [orthonormal](#orthonormal) basis of A consisting of `eigenvectors` of A.

<!--list-separator-->

-  spectral decomposition(eigendecomposition)

    \\(A=QUQ^T\\)

    -   Q: matrix with columns eigenvectors
    -   U: matrix with diagnol eigenvalues corresponding


#### Rayleigh quotients {#rayleigh-quotients}

for a symmetric matrix A
\\(R\_A(x)=\frac{x^TAx}{x^Tx}\\)

<!--list-separator-->

-  propositions

    1.  scale invariance: \\(R\_A(\alpha x)=R\_A(x)\\)
    2.  eigen: if x is a eigenvector of A with eigenvalue \\(\lambda\\), then \\(R\_A(x)=\lambda\\)
    3.  \\(\lambda\_{min}(A) \le x^TAx \le \lambda\_{max}(A)\\), equal when x is eigenvector
    4.  \\(\lambda\_{min}(A) \le R\_A(x) \le \lambda\_{max}(A)\\), equal when x is eigenvector

    <!--list-separator-->

    - <span class="org-todo todo TODO">TODO</span>  prrof of 3,4

<!--list-separator-->

-  quadratic form

    for a symmetric \\(A \in \mathbb{R}^{n \times n}\\), \\(x^TAx\\) is a `quadratic form`


#### Positive semi-definite matrix {#positive-semi-definite-matrix}

a [symmetric matrix](#symmetric-matrix) A is positive semi-definite if:

-   for all \\(x \in \mathbb{R}^n\\), \\(x^TAx \ge 0\\)

<!--list-separator-->

-  positive definite matrix

    a [symmetric matrix](#symmetric-matrix) A is positive semi-definite if:

    -   for all nonzero \\(x \in \mathbb{R}^n\\), \\(x^TAx > 0\\)

    positive definite matrix is subset of positive semi-definite matrix

<!--list-separator-->

-  propositions

    1.  a [symmetric matrix](#symmetric-matrix) is a [Positive semi-definite matrix](#positive-semi-definite-matrix) if and only if:
        -   all its eigenvalues are `nonnegative`
    2.  a [symmetric matrix](#symmetric-matrix) is a [Positive semi-definite matrix](#positive-semi-definite-matrix) if and only if:
        -   all its eigenvalues are `positive`
    3.  [arise] for \\(A \in \mathbb{R}^{m \times n}\\), \\(A^TA\\) is a [Positive semi-definite matrix](#positive-semi-definite-matrix). If \\(null(A) = \\{0\\}\\), then \\(A^TA\\) is a [positive definite matrix](#positive-definite-matrix)
        in the proof, we round \\(A^TA\\) with \\(x^T(...)x\\). Product of a matrix and its transpose.
    4.  [positive definite matrix](#positive-definite-matrix) is invertible (with nonzero eigenvalues).
    5.  [making] for [Positive semi-definite matrix](#positive-semi-definite-matrix) A and positive \\(\alpha\\), \\(A+\alpha I\\) is [positive definite matrix](#positive-definite-matrix)

<!--list-separator-->

-  geometry with [positive definite](#positive-definite-matrix) [quadratic form](#quadratic-form)

    the c-isocontour of function \\(f(x)=x^TAx\\) are ellopsoids such that:

    -   it's axes point in the directions of eigenvectors of A
    -   it's radii of these axes are proportional to the inverse quare roots of corresponding eigenvalues

    <!--list-separator-->

    -  level set

        level set or isocontour.
        \\(\\{x \in dom f \colon f(x)=c\\}\\)
        where a `level` line cut through the function contour.


## singular value decomposition {#singular-value-decomposition}

(SVD)
every matrix \\(A \in \mathbb{R}^{m \times n}\\) has a SVD:
\\(A=U \Sigma V^T\\), where:

-   \\(U \in \mathbb{R}^{m \times m}\\) is [orthogonal matrix](#orthogonal-matrix)
-   \\(V \in \mathbb{R}^{n \times n}\\) is [orthogonal matrix](#orthogonal-matrix)
-   \\(\Sigma \in \mathbb{R}^{m \times n}\\) is a diagonal matrix with:
    -   `sigular values of A` (\\(\sigma\_i\\)) on its diagonal
    -   the entries in non-increasing order(by convention)

anothre notation (with sum of outer product):
\\(A=\sum\_{i=1}^{r}\sigma\_iu\_iv\_i^T\\), where:

-   u_i is ith column of U
-   v_i is ith column of V
-   \\(\sigma\_i\\) is ith singular value


### propositions {#propositions}

-   [valid entry] only `first r = rank(A) singular values` are nonzero.


### eigendecomposition for \\(A^TA\\) and \\(AA^T\\) {#eigendecomposition-for--a-ta--and--aa-t}

\\(A^TA = (U\Sigma V^T)^TU\Sigma V^T = V\Sigma^TU^TU\Sigma V^T=V\Sigma^T\Sigma V^T\\)
\\(AA^T = U\Sigma V^T(U\Sigma V^T)^T =TU\Sigma V^T V\Sigma^TU^=U\Sigma\Sigma^T V^T\\)
Therefore:

-   columns of V are eigenvectors of \\(A^TA\\)
-   columns of U are eigenvectors of \\(AA^T\\)
-   singular values of A are square roost of eigenvalues of \\(A^TA\\)


## Fundamental Theorem of Linear Algebra {#fundamental-theorem-of-linear-algebra}

there are various versions.
for \\(A \in \mathbb{R}^{m \times n}\\):

-   \\(null(A)= range(A^T)^{\perp}\\)
-   \\(null(A) \oplus range(A^T)=\mathbb{R}^n\\)
-   \\(\underbrace{\operatorname{dim} \operatorname{range}(\mathbf{A})}\_{\operatorname{rank}(\mathbf{A})}+\operatorname{dim} \operatorname{null}(\mathbf{A})=n\\)
-   for [singular value decomposition](#singular-value-decomposition) \\(A=U\SigmaV^T\\), columns of U and V form [orthonormal](#orthonormal) bases for the `four fundamental subspaces` of A:
    r = rank(A)

    | subspace   | columns               |
    |------------|-----------------------|
    | range(A)   | first r columns of U  |
    | range(A^T) | first r columns of V  |
    | null(A^T)  | last m-r columns of U |
    | null(A)    | last n-r columns of V |


## operator norm {#operator-norm}

for \\(T \colon V \rightarrow W\\), the operator norm of T is defined as:
\\(\\|T\\|\_{\mathrm{op}}=\max \_{\substack{\mathbf{x} \in V \\\ \mathbf{x} \neq \mathbf{0}}} \frac{\\|T \mathbf{x}\\|\_{W}}{\\|\mathbf{x}\\|\_{V}}\\)


### for \\(R^n \rightarrow R^m\\) {#for--r-n-rightarrow-r-m}

for matrix \\(A \in \mathbb{R}^{m \times n}\\), when p-norm is used in both domain and codomain:

-   matrix p-norm: \\(\\|\mathbf{A}\\|\_{p}=\max \_{\mathbf{x} \neq \mathbf{0}} \frac{\\|\mathbf{A} \mathbf{x}\\|\_{p}}{\\|\mathbf{x}\\|\_{p}}\\)
-   matrix 1-norm:\\(\\|\mathbf{A}\\|\_{1}=\max \_{1 \leq j \leq n} \sum\_{i=1}^{m}\left|A\_{i j}\right|\\)(the largest column sum)
-   matrix \\(\infty - norm\\):\\(\\|\mathbf{A}\\|\_{\infty}=\max \_{1 \leq i \leq m} \sum\_{j=1}^{n}\left|A\_{i j}\right|\\)(the largest row sum)
-   matrix 2-norm: \\(\\|A\\|\_2 =\sigma\_1(A)\\)(the largest sigular value of A)


### Propositions {#propositions}

-   \\(\\|Ax\\|\_p \le \\|A\\|\_P\\|x\\|\_p\\)(by definition)
-   \\(\\|AB\\|\_p \le \\|A\\|\_P\\|B\\|\_p\\)(proof)


### Frobenius norm {#frobenius-norm}

\\(\\|\mathbf{A}\\|\_{\mathrm{F}}=\sqrt{\sum\_{i=1}^{m} \sum\_{j=1}^{n} A\_{i j}^{2}}=\sqrt{\operatorname{tr}\left(\mathbf{A}^{\top} \mathbf{A}\right)}=\sqrt{\sum\_{i=1}^{\min (m, n)} \sigma\_{i}^{2}(\mathbf{A})}\\)


## dimension {#dimension}

dimension is the number of vectors in 1 orthonormal base of V


## invertible {#invertible}