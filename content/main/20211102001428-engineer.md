+++
title = "Engineer"
author = ["System Administrator"]
draft = false
+++

## Computer Science {#computer-science}


### Programming {#programming}


#### Programming languages {#programming-languages}

As a engineer, I need to master some PL to be able to create some accountable work.
Though the philosophy would be the same

<!--list-separator-->

-  C/C++ language

<!--list-separator-->

-  Java

<!--list-separator-->

-  Python

<!--list-separator-->

-  Lisp

    <!--list-separator-->

    -  ACTIVE Clojure

        <!--list-separator-->

        -  how to do things

            in the [clojure]({{< relref "20220105010107-clojure.md" >}}) page. use [my-practice](~/playground/clojure-cookbook/my-practice/1_primitive-data.clj) to hold the .clj files.

        <!--list-separator-->

        -  Use of Lein

        <!--list-separator-->

        -  Graphics

<!--list-separator-->

-  Javascript


#### Algorithms {#algorithms}

<!--list-separator-->

-  PLAN Art of Computer programming

    <!--list-separator-->

    -  Process

        1.  First went through the math very quickly.

            -   outline content
            -   no questions
            -   no eval

            save for further times encountering the real question.
        2.  then skim through the MIX part.
            -   understand the syntax and mechanism
            -   make a cheetsheet
        3.  then go through the data structure part 2 times
            1.  1st time to skim
                -   outline contents
                -   brief explain
            2.  2st time to dig in
                -   find hard points
                -   go back to math and MIX if required.

        <!--list-separator-->

        -  Method

            -   Feynman's technique
            -   Mind map
            -   Spaced Repetition
                use Anki. Break the whole into bite size chunks. just specify the question in the heading of Anki card.

    <!--list-separator-->

    -  New Process!

        <!--list-separator-->

        -  the coding

            use c/java/clojure to write them!

        <!--list-separator-->

        -  the text

            I can read them like read other papers and text!

    <!--list-separator-->

    -  Vol 1: Fundamental Algorithms

        This volumn contains fundamental math, the tutorial of the programming language MIX used in the book, and some data structures.

        The notes are exclusively on Onenote

<!--list-separator-->

-  THREAD Calculus rehab

    I really need to recall some basic calculus techniques. Got some questions and answers

<!--list-separator-->

-  THREAD Machine Learning <span class="tag"><span class="agenda">agenda</span></span>

    <!--list-separator-->

    -  Scheduling Recommandation

        <!--list-separator-->

        -  Time

            None Special.
            Frequent As possible

        <!--list-separator-->

        -  Recource

            Check [INT104]({{< relref "20220224134643-int104.md" >}}), where Dr.Li provided some resouces.
            [Kaggle](https://kaggle.com) have series of tutorials and Practices.

    <!--list-separator-->

    - <span class="org-todo todo TODO">TODO</span>  Kaggle Intro to ML 4/7 Model Validation

        [Model Validation](https://www.kaggle.com/dansbecker/model-validation)

    <!--list-separator-->

    -  THREAD With Math.

        I'd like to check out some math stuff about machine learning

        <!--list-separator-->

        - <span class="org-todo done DONE">DONE</span>  Tackle the class slide -&gt; problem list!

            -   Matrix decomposition
                -   Determinant
                -   Eigenvalue
                -   Singular Value Decomposition
                -   Principle Component Analysis
                -   Independent component Analysis
                -   Non-negative
            -   Continous optimization
                -   gradient desent
                -   constrained optimization
                -   convex optimization
            -   2,4,5,7 chapter of the book Math. for ML
            -   linear algebra words
                -   rank
                -   row/column space
                -   Gram- Schmidt process

        <!--list-separator-->

        - <span class="org-todo done DONE">DONE</span>  Determinant

        <!--list-separator-->

        - <span class="org-todo done DONE">DONE</span>  Eigens

        <!--list-separator-->

        - <span class="org-todo todo TODO">TODO</span>  Sigular Value Decomposition

            [SVD by Steve Brunton](https://www.youtube.com/watch?v=xy3QyyhiuY4#0)a

        <!--list-separator-->

        - <span class="org-todo todo TODO">TODO</span>  Matrix Decomposition.

            Understand every matrix decomposition talked about.

    <!--list-separator-->

    -  THREAD Hands on instruction of data science

        <!--list-separator-->

        -  Schedule Recommendation

            <!--list-separator-->

            -  Time

                I'd suggest As frequent.
                but the materials are messive. Especially the non-technique parts. I suggest 1 chapter each day, as it's an important on.

            <!--list-separator-->

            -  Reference

                _Hands-on Instruction of Data Science_ by Craig Shrah.

            <!--list-separator-->

            -  Organization

        <!--list-separator-->

        - <span class="org-todo todo TODO">TODO</span>  Planning Stuff

            So the Part I covers some concept for understanding the whole concept of _Data Science_, including the `thinking paradigm` `skillset/toolkit` `interfere with other fields` `how data would look like` and data analyse `techniques`. I look forward to how the book would make it concrete and hands-on

            And Part II introducing the tools, covering simple usage of `UNIX` `R` `Python` and `MySQL` in his data analyse process.

            It seems that the author use R as primary language of data analysis. The Part III go through several algorithms of `supervised` and `unsupervised` classifier training using R.

            And the Part IV get into real problems with `using API` to get data, and analyse them. Then talk about how to `choose/evaluate methods`

            1.  Therefore I suggest a very quick overview of everything in Part I, and just go to the tests.
            2.  Afterwards, could play with the tools a little bit, but I guess it won't be long since I have known UNIX, python, and pretty much of MySQL, and how to learn a new language.
            3.  Followed by Part III, it could be tricky, but I see it didn't involve much math, yet still long, so I guess he might have been doing a great job in explaining it. There's still practices I could reflex upon, so no worry. Apply the solving problem application of [Feynman Techinique]({{< relref "20211011020236-feynman_techinique.md" >}}) and I'll be well tended.
            4.  And the Part IV could be very fun because of the real world involed in, and very troublesome, because real world is.

        <!--list-separator-->

        - <span class="org-todo done DONE">DONE</span>  Part I: Conceptual Introduction

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Going over 1. introduction very fast

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Going through the key terms with [Feynman Techinique]({{< relref "20211011020236-feynman_techinique.md" >}})

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Answer the Conceptual questions

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Do the Hands=on Problems

        <!--list-separator-->

        - <span class="org-todo todo TODO">TODO</span>  Part II: Tools for Data Science

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Unix part

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Python part

                just going through it

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Going through the key terms with [Feynman Techinique]({{< relref "20211011020236-feynman_techinique.md" >}})

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Answer the Conceptual questions

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Do the Hands=on Problems

                <!--list-separator-->

                -  Regression practice

                    ```python
                    import pandas as pd
                    import numpy as np
                    import matplotlib.pyplot as plt
                    import statsmodels.api as sm

                    df = pd.read_excel("./resource/mlr05.xls")
                    df.columns=["annual_net_sales","advertising","competitors","X4","X5","X6"]
                    df
                    ```

                    <div class="src-block-caption">
                      <span class="src-block-number">Code Snippet 1</span>:
                      Import
                    </div>

                    ```text
                    # Out[4]:
                    #+BEGIN_EXAMPLE
                      annual_net_sales  advertising  competitors    X4         X5  X6
                      0              231.0          3.0          294   8.2   8.200000  11
                      1              156.0          2.2          232   6.9   4.100000  12
                      2               10.0          0.5          149   3.0   4.300000  15
                      3              519.0          5.5          600  12.0  16.100000   1
                      4              437.0          4.4          567  10.6  14.100000   5
                      5              487.0          4.8          571  11.8  12.700000   4
                      6              299.0          3.1          512   8.1  10.100000  10
                      7              195.0          2.5          347   7.7   8.400000  12
                      8               20.0          1.2          212   3.3   2.100000  15
                      9               68.0          0.6          102   4.9   4.700000   8
                      10             570.0          5.4          788  17.4  12.300000   1
                      11             428.0          4.2          577  10.5  14.000000   7
                      12             464.0          4.7          535  11.3  15.000000   3
                      13              15.0          0.6          163   2.5   2.500000  14
                      14              65.0          1.2          168   4.7   3.300000  11
                      15              98.0          1.6          151   4.6   2.700000  10
                      16             398.0          4.3          342   5.5  16.000000   4
                      17             161.0          2.6          196   7.2   6.300000  13
                      18             397.0          3.8          453  10.4  13.900000   7
                      19             497.0          5.3          518  11.5  16.299999   1
                      20             528.0          5.6          615  12.3  16.000000   0
                      21              99.0          0.8          278   2.8   6.500000  14
                      22               0.5          1.1          142   3.1   1.600000  12
                      23             347.0          3.6          461   9.6  11.300000   6
                      24             341.0          3.5          382   9.8  11.500000   5
                      25             507.0          5.1          590  12.0  15.700000   0
                      26             400.0          8.6          517   7.0  12.000000   8
                    #+END_EXAMPLE
                    ```

                    ```python
                    #1. prepare for lr-model
                    y = df.annual_net_sales
                    X = df[["advertising","competitors"]]
                    X = sm.add_constant(X)
                    lr_model = sm.OLS(y,X).fit()
                    lr_model.summary()
                    ```

                    <div class="src-block-caption">
                      <span class="src-block-number">Code Snippet 2</span>:
                      advertising, competitors -&gt; net sales
                    </div>

                    ```text
                    # Out[11]:
                    #+BEGIN_EXAMPLE
                      <class 'statsmodels.iolib.summary.Summary'>
                      """
                      OLS Regression Results
                      ==============================================================================
                      Dep. Variable:       annual_net_sales   R-squared:                       0.926
                      Model:                            OLS   Adj. R-squared:                  0.920
                      Method:                 Least Squares   F-statistic:                     150.7
                      Date:                Tue, 22 Mar 2022   Prob (F-statistic):           2.59e-14
                      Time:                        21:33:29   Log-Likelihood:                -144.57
                      No. Observations:                  27   AIC:                             295.1
                      Df Residuals:                      24   BIC:                             299.0
                      Df Model:                           2
                      Covariance Type:            nonrobust
                      ===============================================================================
                      coef    std err          t      P>|t|      [0.025      0.975]
                      -------------------------------------------------------------------------------
                      const         -77.8992     24.003     -3.245      0.003    -127.439     -28.359
                      advertising    31.9491      9.860      3.240      0.003      11.599      52.299
                      competitors     0.6664      0.104      6.424      0.000       0.452       0.880
                      ==============================================================================
                      Omnibus:                        3.050   Durbin-Watson:                   1.613
                      Prob(Omnibus):                  0.218   Jarque-Bera (JB):                1.606
                      Skew:                          -0.508   Prob(JB):                        0.448
                      Kurtosis:                       3.630   Cond. No.                         989.
                      ==============================================================================

                      Notes:
                      [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
                      """
                    #+END_EXAMPLE
                    ```

                    ```python
                    from mpl_toolkits.mplot3d import Axes3D
                    X_axis,Y_axis = np.meshgrid([X.advertising.min(),X.advertising.max()],[X.competitors.min(),X.competitors.max()])

                    Z_axis = lr_model.params[0] + lr_model.params[1] * X_axis + lr_model.params[2] * Y_axis
                    fig = plt.figure(figsize = (12,8))
                    ax = Axes3D(fig,azim=-100)
                    ax.plot_surface(X_axis,Y_axis,Z_axis,alpha = 0.5, linewidth=0)

                    ax.scatter(X.advertising,X.competitors,y)
                    ```

                    <div class="src-block-caption">
                      <span class="src-block-number">Code Snippet 3</span>:
                      3D plot
                    </div>

                    &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc37c7f21c0&gt;
                    ![](/ox-hugo/r5IDzm.png)

                <!--list-separator-->

                -  Classification practice

                    uses KNN algorithm:

                    1.  find similar points
                    2.  select the most similar `k` points
                    3.  the `mode` of label of the `k` points is the predicted label.

                    <!--listend-->

                    ```python
                    import numpy as np
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.neighbors import KNeighborsClassifier
                    from sklearn.model_selection  import train_test_split
                    df = pd.read_csv("./resource/wine.csv")
                    X_train,X_test,y_train,y_test = train_test_split(df[["density","sulfates","residual_sugar"]],df["high_quality"],test_size = .3)
                    #make classifers and fit
                    classifier = KNeighborsClassifier(n_neighbors = 3)
                    classifier.fit(X_train,y_train)
                    #test classifers
                    prediction = classifier.predict(X_test)
                    #calculate the correctness
                    correct =  np.where(prediction == y_test,1,0).sum()
                    print(correct)

                    accuracy =  correct/len(y_test)
                    print(accuracy)

                    #plot the accuracy from 1 to 51 step 2
                    results= []
                    for k in range(1,51):
                        classifier = KNeighborsClassifier(n_neighbors = k)
                        classifier.fit(X_train,y_train)
                        prediction = classifier.predict(X_test)
                        accuracy = np.where(prediction == y_test,1,0).sum()/(len(y_test))
                        results.append([k,accuracy])

                    #plotting
                    results = pd.DataFrame(results,columns = ["k","accuracy"])
                    plt.plot(results.k,results.accuracy)
                    plt.title("k vs. accuracy")
                    plt.show()

                    ```

                    {{< figure src="/ox-hugo/Q8E07V.png" >}}

                <!--list-separator-->

                -  Cluster Practice

                    ```python
                    import numpy as np
                    import matplotlib.pyplot as plt
                    import matplotlib.style as stl
                    stl.use("ggplot")

                    #get kmeans
                    from sklearn.cluster import KMeans
                    X = np.array([[1,2],[3,4],[7,2],[11,4],[5,8],[2,9],[4,2]])

                    kmeans = KMeans(n_clusters = 2)
                    kmeans.fit(X)

                    centroids = kmeans.cluster_centers_
                    labels = kmeans.labels_

                    centroids, labels

                    colors = ["g.","r.","c.","y."]

                    for i in range(len(X)):
                        plt.plot(X[i][0],X[i][1],colors[labels[i]],markersize = 10)


                    plt.scatter(centroids[:,0],centroids[:,1], marker = "x",s = 150,linewidth=2,zorder=10)

                    plt.show()
                    ```

                    {{< figure src="/ox-hugo/GqpjGC.png" >}}

                <!--list-separator-->

                -  Density estimation

                    ```python
                    import numpy as np
                    from sklearn.cluster import MeanShift
                    from sklearn.datasets.samples_generator import make_blobs
                    import matplotlib.pyplot as plt
                    from mpl_toolkits.mplot3d import Axes3D
                    import matplotlib.style as stl
                    stl.use("ggplot")
                    centers = [[1,1,1],[5,5,5],[10,10,10]]
                    X,y = make_blobs(n_samples= 100,centers= centers,cluster_std= 2)
                    ms = MeanShift()
                    ms.fit(X)
                    centroids = ms.cluster_centers_
                    labels = ms.labels_
                    n_clusters_ = len(np.unique(labels))
                    print("estimated clusters: " ,n_clusters_)

                    colors= ["r","g","b","c","y","m","k"]

                    fig =  plt.figure()
                    ax = fig.add_subplot(111,projection = "3d")

                    for i in range(len(X)):
                        ax.scatter(X[i][0],X[i][1],X[i][2],c = colors[labels[i]],marker ="o")
                    ax.scatter(centroids[:,0],centroids[:,1],centroids[:,2], marker = "x", s = 150, linewidth=5, zorder=10)
                    plt.show()
                    ```

                    {{< figure src="/ox-hugo/HTCqHs.png" >}}

                <!--list-separator-->

                -  Python

                    <!--list-separator-->

                    -  Stat

                        ```python
                        import numpy as np
                        X = np.array([164, 158, 172, 153, 144, 156, 189, 163, 134, 159, 143, 176, 177, 162, 141, 151, 182, 185, 171, 152])
                        import matplotlib.pyplot as plt
                        plt.figure()
                        hist1,edges1 = np.histogram(X,bins=4)
                        plt.bar(edges1[:-1],hist1,width = edges1[1:]-edges1[:-1])
                        plt.show()
                        return np.mean(X),np.median(X),np.std(X)

                        ```

                    <!--list-separator-->

                    -  Boston

                        ```python
                        import numpy as np
                        import matplotlib.pyplot as plt
                        import pandas as pd
                        import statsmodels.api as sm
                        df = pd.read_csv("~/playground/General Resources/Data and code/Datasets/OA 5.7 - boston.csv")
                        df.describe()

                        X = df["AGE"]
                        y = df["NOX"]
                        X = sm.add_constant(X)

                        linear_model = sm.OLS(y,X).fit()
                        linear_model.rsquared
                        rsquares = []
                        for column in df.columns:
                             X = df[column]
                             y = df["NOX"]
                             X = sm.add_constant(X)
                             linear_model = sm.OLS(y,X).fit()
                             rsquares.append(linear_model.rsquared)
                        result = list(zip(df.columns,rsquares))
                        result
                        ```

                        ```text
                        # Out[20]:
                        #+BEGIN_EXAMPLE
                          [('CRIM', 0.17721718179269352),
                          ('ZN', 0.26687939094162105),
                          ('NDUS', 0.5831635323844071),
                          ('CHAS', 0.008317951975949422),
                          ('NOX', 1.0),
                          ('RM', 0.09131770087582114),
                          ('AGE', 0.535048512732641),
                          ('DIS', 0.5917149670934201),
                          ('RAD', 0.37385956267556064),
                          ('TAX', 0.44625499627669685),
                          ('PTRATIO', 0.035695556480997426),
                          ('B', 0.1444384872864103),
                          ('LSTAT', 0.3491378991413132),
                          ('MEDV', 0.1826030425016989)]
                        #+END_EXAMPLE
                        ```

                        ```python
                        X = df.DIS
                        y = df.NOX

                        X = sm.add_constant(X)
                        X_axis = np.linspace(X.DIS.min(),X.DIS.max(),100)
                        X_axis = sm.add_constant(X_axis)
                        linear_model = sm.OLS(y,X).fit()
                        y_hat = linear_model.predict(X_axis)
                        plt.figure()
                        plt.plot(X_axis,y_hat)
                        plt.show()
                        print("the equation is :", "y = ", linear_model.params[0], " + ", linear_model.params[1], " * x" )
                        ```

                        ```text
                        # Out[36]:
                        [[file:./obipy-resources/Oywtol.png]]
                        ```

                        ![](/ox-hugo/cNPPVS.png)
                        ![](/ox-hugo/w9tl7I.png)

                    <!--list-separator-->

                    -  IRIS

                        ```python
                        import numpy as np
                        import matplotlib.pyplot as plt
                        import pandas as pd
                        import matplotlib.style as stl
                        stl.use("ggplot")
                        from sklearn.neighbors import KNeighborsClassifier
                        from sklearn.svm import SVC
                        from sklearn import datasets
                        from sklearn.model_selection import train_test_split

                        iris = datasets.load_iris()
                        df = pd.DataFrame(iris.data)
                        df["class"] = iris.target
                        df.columns = ["Sepal length","Sepal width","Pedal langth","Pedal width","Class"]

                        X_train,X_test,y_train,y_test = train_test_split(df[["Sepal length","Sepal width"]],df["Class"],test_size = .3)

                        classifier = KNeighborsClassifier(n_neighbors = 5)
                        classifier.fit(X_train,y_train)

                        prediction = classifier.predict(X_test)
                        correct = np.where(prediction == y_test, 1,0).sum()

                        accuracy = correct/(len(y_test))
                        accuracy




                        #kmeans = KNeighborsClassifier(n_neighbors = 4)
                        #kmeans.fit()
                        ```

                        <div class="src-block-caption">
                          <span class="src-block-number">Code Snippet 4</span>:
                          KNN
                        </div>

                        ```text
                        # Out[19]:
                        : 0.7111111111111111
                        ```

                        ```python

                        svm_kernel_types = ["linear","rbf","poly"]
                        svm = SVC(kernel = "poly")
                        svm.fit(X_train,y_train)

                        prediction = svm.predict(X_test)
                        correct = np.where(prediction == y_test,1,0).sum()
                        accuracy = correct/(len(y_test))
                        accuracy

                        ```

                        <div class="src-block-caption">
                          <span class="src-block-number">Code Snippet 5</span>:
                          SVM
                        </div>

                        ```text
                        # Out[25]:
                        : 0.7555555555555555
                        ```

                        ```python
                        from sklearn.cluster import KMeans
                        kmeans = KMeans(n_clusters = 3)
                        kmeans.fit(df[["Sepal length","Sepal width"]])

                        centroids,labels = kmeans.cluster_centers_,kmeans.labels_
                        colors = ["g","r","c","y."]

                        for i in range(len(df)):
                             plt.scatter(df["Sepal length"][i],df["Sepal width"][i],c = colors[labels[i]])

                        plt.scatter(centroids[:,0],centroids[:,1],marker = "x",s = 150, linewidth = 2, zorder = 10)
                        df["Sepal length"][2]
                        ```

                        <div class="src-block-caption">
                          <span class="src-block-number">Code Snippet 6</span>:
                          Cluster
                        </div>

                        ```text
                        # Out[44]:
                        : 4.7
                        ```

                        {{< figure src="/ox-hugo/MEzL5y.png" >}}

                    <!--list-separator-->

                    -  Breast Cancer

                        ```python
                        import numpy as np
                        import matplotlib.pyplot as plt
                        from sklearn.cluster import KMeans
                        import pandas as pd
                        df = pd.read_csv("/Users/hermanhe/playground/General Resources/Data and code/Datasets/dataR2.csv")
                        type(df["Leptin"][1])
                        df["Leptin"] = list(map(lambda x:int(x * 100)/100,df["Leptin"]))
                        X = df[df.columns[:9]]
                        kmeans = KMeans(n_clusters = 2)
                        kmeans.fit(X)
                        labels = kmeans.labels_

                        accuracy = np.where(labels!=df.Classification,1,0).sum()/(len(labels))
                        accuracy



                        ```

                        ```text
                        # Out[41]:
                        : 0.896551724137931
                        ```

        <!--list-separator-->

        - <span class="org-todo todo TODO">TODO</span>  Part III: Machine Learning for Data Science

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Going through the key terms with [Feynman Techinique]({{< relref "20211011020236-feynman_techinique.md" >}})

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Answer the Conceptual questions

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Do the Hands=on Problems

        <!--list-separator-->

        - <span class="org-todo todo TODO">TODO</span>  Part IV: Application, evaluations, and methods.

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Going through the key terms with [Feynman Techinique]({{< relref "20211011020236-feynman_techinique.md" >}})

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Answer the Conceptual questions

            <!--list-separator-->

            - <span class="org-todo todo TODO">TODO</span>  Do the Hands=on Problems

<!--list-separator-->

-  THREAD Project Euler <span class="tag"><span class="agenda">agenda</span></span>

    <!--list-separator-->

    -  Scheduling Recommandation

        <!--list-separator-->

        -  Time

            none
            could frequently.

        <!--list-separator-->

        -  Recource

            [Project Euler](https://projecteuler.net)

    <!--list-separator-->

    - <span class="org-todo done DONE">DONE</span>  Prob. 9

        [code](~/playground/clojure/.#project_euler.clj)
        use algebra.
        a = 2mn, b = m2-n2, c = m2+ n2.
        ==&gt; m&gt;n&gt;0, m(m+n)=500. 500 = 2 \* 2 \* 5 \* 5 \* 5

        | m   | m+n | correct? |
        |-----|-----|----------|
        | 2   | 250 | no       |
        | 4   | 125 | no       |
        | 20  | 25  | yes      |
        | 100 | 5   | no       |
        | 500 | 1   | no       |
        | 10  | 50  | no       |

    <!--list-separator-->

    - <span class="org-todo todo TODO">TODO</span>  Prob. 10

        sum of primes below 2,000,000

    <!--list-separator-->

    -  Prob.13

        The brute force. And a slightly performance version.
        Both use the long-add function.


### Coding {#coding}


#### Habit contract {#habit-contract}

I like coding, and I want be better at coding and engineering, I want to be an exellent hacker, so I code everyday, with conscious and wise mind.

I shall code with my wits, my integrity, and my dedicated loyalty.
I shall not code with distracting, nor code dumb without thinking
I shall log everything I see and thought about them properly.
--&gt; I shall code to the standard of [coding machine]({{< relref "20220326155217-coding_machine.md" >}})

And if I haven't achieve all the descriptions above, I shall add 2 more minutes of running/10 more pull up this day. Record in the day's page.

<!--list-separator-->

-  + coding(to a book/a problem I encounter)

    <!--list-separator-->

    -  Cue - obvious

        -   Habit stack: I'll start coding, Right after I take my morning dump. I could use toilet time to think/deside what to do
        -   environment: I ahve ACP side of my desk.

    <!--list-separator-->

    -  Craving - attractive

        -   Reprogramming the brain
            -   the benefit of improving my coding skill:
                1.  I'll be more capable in engineering.
                2.  I'll be powerful
                3.  People would admire me
                4.  I'll have a lot of fun in programming and engineering a system
                5.  It'll enable me to eventually build my baby AI
                6.  The whole thing of coding/engineering would be more clear to me, instead of vague and frustrating mistery
        -   Temptation bundling
            After I've done a session of coding, I'll practice a knot

    <!--list-separator-->

    -  Response - easy

        -   least effort:
            I just have to code for 1 session to whatever material at hand.
            It could be from the _Hands-on intro to ML_ book, or a projectEuler challenge, or a random program from my todo list, anything would be suffice
        -   Prime the environment:
            Choose a coding task before I went to toilet, and open the file I needed.
        -   Use a 2-minuet version:
            Do 2-minute coding first if procrastinate. But I doubt if I will.

    <!--list-separator-->

    -  Reward - satisfying

        -   Adding little pleasure
            After the session, tie a knot.
            They are both engineering &amp; art
        -   Habit track
            Make a subtree coding in Engineering, style habit. Each day ofter the 1 session, set it marked.
        -   Habit contract
            In the heading, make declaims
        -   Visual measurements
            log every thing done. Round thing up

    <!--list-separator-->

    -  Mindset - continue process

        -   I'm a hacker
        -   1% improvement a day.
        -   showing up's half the bottle


#### logs {#logs}

<!--list-separator-->

-  2022

    <!--list-separator-->

    -  2022-03 March

        <!--list-separator-->

        -  2022-03-26 Saturday

            <!--list-separator-->

            -  11:10 AM - coding on  org-capture in chrome

        <!--list-separator-->

        -  2022-03-27 Sunday

            <!--list-separator-->

            -  10:52 AM - coding on  ML

                [Classification practice](#classification-practice)

                doing KNN of classification practice with _Hands-on Intro of ML_
                Then applied it to the CW_Data.csv data

            <!--list-separator-->

            -  02:18 PM - coding on  ML-cluster

                [Cluster: use Q1 and Q2]({{< relref "20220224134643-int104#cluster-use-q1-and-q2" >}})

                sklearn.Cluster.KMeans 里的attributes很多后面带一个_, cluster_centers\_, labels_之类的.

                use `zip` to zip 2 columns (with pandas, very handy)

                Apply the technique to CW2 Q1 and Q2, very neat clustering. Very different with the real cluster with label programme, which is interleaved.

            <!--list-separator-->

            -  02:51 PM - coding on  density estimation

                Doing a dencity estimation in _Hands-on_
                have the results not in a drawer, but prefixed by `:` can have the history not edited.(if I hand remove the `:`)
                When with CW_Data, 1 looks similar.

            <!--list-separator-->

            -  04:34 PM - coding on  python problems

                going through the problems in _Hands-on_

    <!--list-separator-->

    -  2022-04 April

        <!--list-separator-->

        -  2022-04-12 Tuesday