<!doctype html><html><title>Engineer</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name=apple-mobile-web-app-capable content="yes"><link rel=stylesheet href=https://hermanhel.github.io/braindump/css/main.min.291920189b1189fd3388c38fd9e8ccbdf24640f05f020c32e3da5ba83f9bd5d8.css><body><header><a href=/ id=logo><svg id="Capa_1" enable-background="new 0 0 511.992 511.992" height="512" viewBox="0 0 511.992 511.992" width="512" xmlns="http://www.w3.org/2000/svg"><g><g><g><path d="m256 420.826c0 38.345-11.844 68.545-49.991 68.014-27.744-.385-51.481-15.31-61.853-39.46-1.239-2.887-4.024-4.734-7.154-4.725-.07.0-.135.0-.201.0-47.474.0-75.537-26.171-75.537-73.882.0-5.633.542-11.138 1.568-16.468.62-3.229-.825-6.489-3.671-8.125C23.668 325.748 10 300.053 10 255.997c0-44.057 13.668-69.757 49.161-90.185 2.846-1.636 4.291-4.896 3.671-8.13-1.026-5.33-1.568-10.83-1.568-16.463.0-47.711 28.064-73.882 75.537-73.882h.201c3.13.009 5.915-1.837 7.154-4.729 10.372-24.145 34.109-39.069 61.853-39.455C244.159 22.621 256 52.821 256 91.166" fill="#ff9eb1"/></g><g><g><g><path d="m256 91.166c0-38.344 11.844-68.545 49.991-68.014 27.744.385 51.481 15.31 61.853 39.46 1.239 2.887 4.024 4.734 7.154 4.724h.201c47.474.0 75.537 26.171 75.537 73.882.0 5.633-.542 11.138-1.568 16.468-.62 3.229.825 6.489 3.671 8.125C488.332 186.245 502 211.939 502 255.996s-13.668 69.756-49.161 90.185c-2.846 1.636-4.291 4.896-3.671 8.13 1.026 5.33 1.568 10.83 1.568 16.463.0 47.711-28.064 73.882-75.537 73.882h-.201c-3.13-.009-5.915 1.837-7.154 4.729-10.372 24.145-34.109 39.069-61.853 39.455-38.15.531-49.991-29.669-49.991-68.014" fill="#ff7d97"/></g></g><g><g><path d="m502 265.996c-4.193.0-7.984-2.713-9.407-6.636-1.419-3.912-.16-8.459 3.063-11.092 3.291-2.689 8.009-2.99 11.621-.758 3.568 2.205 5.404 6.578 4.478 10.669-1.02 4.501-5.126 7.817-9.755 7.817z"/></g></g></g><g><path d="m340.83 229.18h-58.013v-58.014h-53.634v58.014H171.17v53.633h58.013v58.013h53.634v-58.013h58.013z" fill="#faf7f5"/></g></g><g><g><path d="m498.468 291.859c-5.141-2.02-10.945.508-12.965 5.648-6.442 16.389-18.055 28.727-37.649 40.005-6.513 3.746-9.932 11.253-8.505 18.689.921 4.783 1.388 9.686 1.388 14.572.0 41.792-22.662 63.882-65.537 63.882h-.225c-.938.0-1.864.074-2.771.217-15.031-4.92-23.796-20.93-19.661-36.479 1.42-5.337-1.757-10.815-7.094-12.234-5.333-1.418-10.814 1.756-12.234 7.094-5.958 22.405 4.241 45.396 23.384 56.443-9.583 17.791-28.602 28.836-50.748 29.145-11.303.146-19.802-2.743-26.011-8.867-9.184-9.057-13.84-25.592-13.84-49.148v-70h16.816c5.522.0 10-4.477 10-10v-48.014h48.014c5.522.0 10-4.477 10-10V229.18c0-5.523-4.478-10-10-10h-48.014v-48.014c0-5.523-4.478-10-10-10H266v-70c0-23.555 4.657-40.09 13.841-49.148 6.21-6.123 14.696-9.022 26.011-8.867 23.862.332 44.096 13.132 52.803 33.405.218.509.458 1.003.719 1.483-3.225 8.243-9.084 15.093-16.833 19.574-8.993 5.2-19.465 6.581-29.485 3.89-5.337-1.434-10.819 1.73-12.251 7.064-1.433 5.334 1.729 10.819 7.063 12.252 5.074 1.363 10.221 2.037 15.338 2.037 10.2.0 20.272-2.681 29.346-7.928 11.083-6.408 19.607-16.017 24.616-27.575 41.6.67 63.569 22.718 63.569 63.866.0 4.89-.467 9.794-1.389 14.582-1.427 7.426 1.991 14.933 8.502 18.678 19.572 11.267 31.176 23.584 37.625 39.936 1.552 3.934 5.318 6.334 9.306 6.333 1.221.0 2.462-.225 3.666-.7 5.138-2.026 7.66-7.834 5.634-12.972-7.943-20.141-22.201-35.773-44.801-49.086.967-5.521 1.457-11.155 1.457-16.772.0-52.114-31.48-83.391-84.288-83.876-12.157-26.863-38.963-43.753-70.318-44.189-16.67-.232-30.255 4.688-40.332 14.625-3.813 3.76-7.08 8.226-9.797 13.382-2.717-5.156-5.984-9.622-9.797-13.382-10.075-9.937-23.668-14.852-40.333-14.625-31.353.437-58.158 17.325-70.32 44.189-52.807.485-84.286 31.762-84.286 83.876.0 5.617.49 11.253 1.458 16.771-37.422 22.031-52.724 50.552-52.724 98.008.0 47.451 15.299 75.969 52.721 98.006-.967 5.521-1.457 11.154-1.457 16.772.0 52.114 31.48 83.391 84.288 83.876 12.157 26.863 38.963 43.753 70.318 44.189.377.005.751.008 1.125.008 16.172.0 29.358-4.92 39.207-14.632 3.813-3.76 7.08-8.226 9.797-13.382 2.717 5.156 5.984 9.622 9.797 13.382 9.849 9.713 23.034 14.633 39.208 14.632.373.0.749-.003 1.125-.008 31.352-.436 58.158-17.325 70.32-44.189 52.807-.485 84.286-31.762 84.286-83.876.0-5.617-.49-11.253-1.458-16.772 22.634-13.329 36.901-28.989 44.838-49.178 2.022-5.14-.507-10.945-5.647-12.966zM282.816 239.18h48.014v33.633h-48.014c-5.522.0-10 4.477-10 10v48.014h-33.633v-48.014c0-5.523-4.477-10-10-10H181.17V239.18h48.014c5.523.0 10-4.477 10-10v-48.014h33.633v48.014c-.001 5.523 4.477 10 9.999 10zm-50.657 230.794c-6.21 6.124-14.717 9.018-26.011 8.867-23.862-.331-44.096-13.132-52.803-33.405-.218-.509-.458-1.003-.719-1.483 3.225-8.243 9.085-15.093 16.833-19.574 8.992-5.2 19.463-6.581 29.485-3.89 5.337 1.434 10.819-1.73 12.251-7.064 1.433-5.333-1.73-10.819-7.064-12.251-15.188-4.08-31.059-1.988-44.684 5.891-11.083 6.408-19.607 16.017-24.616 27.575-41.6-.67-63.569-22.718-63.569-63.866.0-4.89.467-9.794 1.389-14.582 1.427-7.427-1.991-14.934-8.502-18.678-32.183-18.528-44.149-40.621-44.149-81.517.0-40.9 11.966-62.994 44.146-81.517 6.513-3.746 9.932-11.253 8.505-18.689-.921-4.783-1.388-9.686-1.388-14.572.0-41.792 22.662-63.882 65.537-63.882h.225c.938.0 1.864-.074 2.771-.217 15.031 4.92 23.796 20.93 19.661 36.479-1.42 5.337 1.757 10.815 7.094 12.234.861.229 1.726.338 2.577.338 4.422.0 8.467-2.956 9.657-7.432 5.958-22.405-4.241-45.396-23.384-56.443 9.583-17.791 28.602-28.836 50.748-29.145 11.267-.15 19.801 2.743 26.011 8.867 9.184 9.057 13.84 25.592 13.84 49.148v70h-16.816c-5.522.0-10 4.477-10 10v48.014H171.17c-5.522.0-10 4.477-10 10v53.633c0 5.523 4.478 10 10 10h48.014v48.014c0 5.523 4.478 10 10 10H246v70c0 23.554-4.657 40.09-13.841 49.147z"/><path d="m139.699 228.227c-6.766.0-13.186 1.514-18.907 4.31-3.049-8.65-8.286-16.485-15.336-22.673-4.151-3.643-10.469-3.233-14.113.918-3.643 4.15-3.232 10.469.918 14.112 6.711 5.891 10.816 14.143 11.498 22.953-1.213 1.914-2.293 3.946-3.225 6.088-1.145 2.633-2.015 5.316-2.615 8.019-13.414 11.422-33.601 10.834-46.225-1.792-3.906-3.904-10.236-3.904-14.143.0-3.905 3.905-3.905 10.237.0 14.143 10.524 10.524 24.354 15.784 38.193 15.784 8.04.0 16.083-1.775 23.484-5.325 1.904 5.443 4.967 10.557 9.136 15.03.596.639 1.204 1.269 1.826 1.891 1.953 1.953 4.512 2.929 7.071 2.929 2.56.0 5.118-.976 7.071-2.929 3.905-3.905 3.905-10.237.0-14.143-.458-.458-.906-.922-1.342-1.389-6.26-6.716-7.799-15.778-4.118-24.241 2.878-6.616 9.86-13.686 20.826-13.686 5.522.0 10-4.477 10-10 .001-5.522-4.476-9.999-9.999-9.999z"/><path d="m387.667 287.543c-3.905 3.905-3.905 10.237.0 14.143 1.953 1.953 4.512 2.929 7.071 2.929s5.118-.976 7.071-2.929c.622-.622 1.23-1.253 1.83-1.896 4.167-4.471 7.229-9.583 9.133-15.025 7.401 3.549 15.444 5.324 23.484 5.324 13.839.0 27.67-5.261 38.193-15.784 3.905-3.905 3.905-10.237.0-14.143-3.906-3.904-10.236-3.904-14.143.0-12.624 12.625-32.811 13.214-46.225 1.792-.6-2.702-1.47-5.386-2.615-8.019-.932-2.142-2.012-4.175-3.225-6.088.682-8.81 4.787-17.062 11.498-22.953 4.15-3.644 4.561-9.962.918-14.112-3.646-4.151-9.964-4.563-14.113-.918-7.05 6.189-12.287 14.023-15.336 22.673-5.721-2.796-12.141-4.31-18.907-4.31-5.523.0-10 4.477-10 10s4.477 10 10 10c10.966.0 17.948 7.07 20.826 13.686 3.681 8.463 2.142 17.525-4.114 24.237-.44.47-.888.935-1.346 1.393z"/></g></g></g></svg></a><h3 class=site-title>Herman's Place</h3><form id=search action=https://hermanhel.github.io/braindump/search/ method=get><label hidden for=search-input>Search site</label>
<input type=text id=search-input name=query placeholder="Type here to search">
<input type=submit value=search></form></header><div class=grid-container><div class=grid><div class=page data-level=1><div class=content><h1>Engineer</h1><h2 id=computer-science>Computer Science</h2><h3 id=programming>Programming</h3><h4 id=programming-languages>Programming languages</h4><p>As a engineer, I need to master some PL to be able to create some accountable work.
Though the philosophy would be the same</p><ul><li>C/C++ language</li></ul><ul><li>Java</li></ul><ul><li>Python</li></ul><ul><li><p>Lisp</p><ul><li><p>ACTIVE Clojure</p><ul><li><p>how to do things</p><p>in the <a href=/braindump/main/20220105010107-clojure/>clojure</a> page. use <a href=~/playground/clojure-cookbook/my-practice/1_primitive-data.clj>my-practice</a> to hold the .clj files.</p></li></ul><ul><li>Use of Lein</li></ul><ul><li>Graphics</li></ul></li></ul></li></ul><ul><li>Javascript</li></ul><h4 id=algorithms>Algorithms</h4><ul><li><p>PLAN Art of Computer programming</p><ul><li><p>Process</p><ol><li><p>First went through the math very quickly.</p><ul><li>outline content</li><li>no questions</li><li>no eval</li></ul><p>save for further times encountering the real question.</p></li><li><p>then skim through the MIX part.</p><ul><li>understand the syntax and mechanism</li><li>make a cheetsheet</li></ul></li><li><p>then go through the data structure part 2 times</p><ol><li>1st time to skim<ul><li>outline contents</li><li>brief explain</li></ul></li><li>2st time to dig in<ul><li>find hard points</li><li>go back to math and MIX if required.</li></ul></li></ol></li></ol><ul><li><p>Method</p><ul><li>Feynman&rsquo;s technique</li><li>Mind map</li><li>Spaced Repetition
use Anki. Break the whole into bite size chunks. just specify the question in the heading of Anki card.</li></ul></li></ul></li></ul><ul><li><p>New Process!</p><ul><li><p>the coding</p><p>use c/java/clojure to write them!</p></li></ul><ul><li><p>the text</p><p>I can read them like read other papers and text!</p></li></ul></li></ul><ul><li><p>Vol 1: Fundamental Algorithms</p><p>This volumn contains fundamental math, the tutorial of the programming language MIX used in the book, and some data structures.</p><p>The notes are exclusively on Onenote</p></li></ul></li></ul><ul><li><p>THREAD Calculus rehab</p><p>I really need to recall some basic calculus techniques. Got some questions and answers</p></li></ul><ul><li><p>THREAD Machine Learning agenda</p><ul><li><p>Scheduling Recommandation</p><ul><li><p>Time</p><p>None Special.
Frequent As possible</p></li></ul><ul><li><p>Recource</p><p>Check <a href=/braindump/main/20220224134643-int104/>INT104</a>, where Dr.Li provided some resouces.
<a href=https://kaggle.com>Kaggle</a> have series of tutorials and Practices.</p></li></ul></li></ul><ul><li><p>TODO Kaggle Intro to ML 4/7 Model Validation</p><p><a href=https://www.kaggle.com/dansbecker/model-validation>Model Validation</a></p></li></ul><ul><li><p>THREAD With Math.</p><p>I&rsquo;d like to check out some math stuff about machine learning</p><ul><li><p>DONE Tackle the class slide -> problem list!</p><ul><li>Matrix decomposition<ul><li>Determinant</li><li>Eigenvalue</li><li>Singular Value Decomposition</li><li>Principle Component Analysis</li><li>Independent component Analysis</li><li>Non-negative</li></ul></li><li>Continous optimization<ul><li>gradient desent</li><li>constrained optimization</li><li>convex optimization</li></ul></li><li>2,4,5,7 chapter of the book Math. for ML</li><li>linear algebra words<ul><li>rank</li><li>row/column space</li><li>Gram- Schmidt process</li></ul></li></ul></li></ul><ul><li>DONE Determinant</li></ul><ul><li>DONE Eigens</li></ul><ul><li><p>TODO Sigular Value Decomposition</p><p><a href="https://www.youtube.com/watch?v=xy3QyyhiuY4#0">SVD by Steve Brunton</a>a</p></li></ul><ul><li><p>TODO Matrix Decomposition.</p><p>Understand every matrix decomposition talked about.</p></li></ul></li></ul><ul><li><p>THREAD Hands on instruction of data science</p><ul><li><p>Schedule Recommendation</p><ul><li><p>Time</p><p>I&rsquo;d suggest As frequent.
but the materials are messive. Especially the non-technique parts. I suggest 1 chapter each day, as it&rsquo;s an important on.</p></li></ul><ul><li><p>Reference</p><p><em>Hands-on Instruction of Data Science</em> by Craig Shrah.</p></li></ul><ul><li>Organization</li></ul></li></ul><ul><li><p>TODO Planning Stuff</p><p>So the Part I covers some concept for understanding the whole concept of <em>Data Science</em>, including the <code>thinking paradigm</code> <code>skillset/toolkit</code> <code>interfere with other fields</code> <code>how data would look like</code> and data analyse <code>techniques</code>. I look forward to how the book would make it concrete and hands-on</p><p>And Part II introducing the tools, covering simple usage of <code>UNIX</code> <code>R</code> <code>Python</code> and <code>MySQL</code> in his data analyse process.</p><p>It seems that the author use R as primary language of data analysis. The Part III go through several algorithms of <code>supervised</code> and <code>unsupervised</code> classifier training using R.</p><p>And the Part IV get into real problems with <code>using API</code> to get data, and analyse them. Then talk about how to <code>choose/evaluate methods</code></p><ol><li>Therefore I suggest a very quick overview of everything in Part I, and just go to the tests.</li><li>Afterwards, could play with the tools a little bit, but I guess it won&rsquo;t be long since I have known UNIX, python, and pretty much of MySQL, and how to learn a new language.</li><li>Followed by Part III, it could be tricky, but I see it didn&rsquo;t involve much math, yet still long, so I guess he might have been doing a great job in explaining it. There&rsquo;s still practices I could reflex upon, so no worry. Apply the solving problem application of <a href=/braindump/main/20211011020236-feynman_techinique/>Feynman Techinique</a> and I&rsquo;ll be well tended.</li><li>And the Part IV could be very fun because of the real world involed in, and very troublesome, because real world is.</li></ol></li></ul><ul><li><p>DONE Part I: Conceptual Introduction</p><ul><li>TODO Going over 1. introduction very fast</li></ul><ul><li>TODO Going through the key terms with <a href=/braindump/main/20211011020236-feynman_techinique/>Feynman Techinique</a></li></ul><ul><li>TODO Answer the Conceptual questions</li></ul><ul><li>TODO Do the Hands=on Problems</li></ul></li></ul><ul><li><p>TODO Part II: Tools for Data Science</p><ul><li>TODO Unix part</li></ul><ul><li><p>TODO Python part</p><p>just going through it</p></li></ul><ul><li>TODO Going through the key terms with <a href=/braindump/main/20211011020236-feynman_techinique/>Feynman Techinique</a></li></ul><ul><li>TODO Answer the Conceptual questions</li></ul><ul><li><p>TODO Do the Hands=on Problems</p><ul><li><p>Regression practice</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
<span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>import</span> statsmodels.api <span style=color:#f92672>as</span> sm

df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_excel(<span style=color:#e6db74>&#34;./resource/mlr05.xls&#34;</span>)
df<span style=color:#f92672>.</span>columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;annual_net_sales&#34;</span>,<span style=color:#e6db74>&#34;advertising&#34;</span>,<span style=color:#e6db74>&#34;competitors&#34;</span>,<span style=color:#e6db74>&#34;X4&#34;</span>,<span style=color:#e6db74>&#34;X5&#34;</span>,<span style=color:#e6db74>&#34;X6&#34;</span>]
df
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[4]:
#+BEGIN_EXAMPLE
  annual_net_sales  advertising  competitors    X4         X5  X6
  0              231.0          3.0          294   8.2   8.200000  11
  1              156.0          2.2          232   6.9   4.100000  12
  2               10.0          0.5          149   3.0   4.300000  15
  3              519.0          5.5          600  12.0  16.100000   1
  4              437.0          4.4          567  10.6  14.100000   5
  5              487.0          4.8          571  11.8  12.700000   4
  6              299.0          3.1          512   8.1  10.100000  10
  7              195.0          2.5          347   7.7   8.400000  12
  8               20.0          1.2          212   3.3   2.100000  15
  9               68.0          0.6          102   4.9   4.700000   8
  10             570.0          5.4          788  17.4  12.300000   1
  11             428.0          4.2          577  10.5  14.000000   7
  12             464.0          4.7          535  11.3  15.000000   3
  13              15.0          0.6          163   2.5   2.500000  14
  14              65.0          1.2          168   4.7   3.300000  11
  15              98.0          1.6          151   4.6   2.700000  10
  16             398.0          4.3          342   5.5  16.000000   4
  17             161.0          2.6          196   7.2   6.300000  13
  18             397.0          3.8          453  10.4  13.900000   7
  19             497.0          5.3          518  11.5  16.299999   1
  20             528.0          5.6          615  12.3  16.000000   0
  21              99.0          0.8          278   2.8   6.500000  14
  22               0.5          1.1          142   3.1   1.600000  12
  23             347.0          3.6          461   9.6  11.300000   6
  24             341.0          3.5          382   9.8  11.500000   5
  25             507.0          5.1          590  12.0  15.700000   0
  26             400.0          8.6          517   7.0  12.000000   8
#+END_EXAMPLE
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>#1. prepare for lr-model</span>
y <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>annual_net_sales
X <span style=color:#f92672>=</span> df[[<span style=color:#e6db74>&#34;advertising&#34;</span>,<span style=color:#e6db74>&#34;competitors&#34;</span>]]
X <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>add_constant(X)
lr_model <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>OLS(y,X)<span style=color:#f92672>.</span>fit()
lr_model<span style=color:#f92672>.</span>summary()
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[11]:
#+BEGIN_EXAMPLE
  &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
  &#34;&#34;&#34;
  OLS Regression Results
  ==============================================================================
  Dep. Variable:       annual_net_sales   R-squared:                       0.926
  Model:                            OLS   Adj. R-squared:                  0.920
  Method:                 Least Squares   F-statistic:                     150.7
  Date:                Tue, 22 Mar 2022   Prob (F-statistic):           2.59e-14
  Time:                        21:33:29   Log-Likelihood:                -144.57
  No. Observations:                  27   AIC:                             295.1
  Df Residuals:                      24   BIC:                             299.0
  Df Model:                           2
  Covariance Type:            nonrobust
  ===============================================================================
  coef    std err          t      P&gt;|t|      [0.025      0.975]
  -------------------------------------------------------------------------------
  const         -77.8992     24.003     -3.245      0.003    -127.439     -28.359
  advertising    31.9491      9.860      3.240      0.003      11.599      52.299
  competitors     0.6664      0.104      6.424      0.000       0.452       0.880
  ==============================================================================
  Omnibus:                        3.050   Durbin-Watson:                   1.613
  Prob(Omnibus):                  0.218   Jarque-Bera (JB):                1.606
  Skew:                          -0.508   Prob(JB):                        0.448
  Kurtosis:                       3.630   Cond. No.                         989.
  ==============================================================================

  Notes:
  [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
  &#34;&#34;&#34;
#+END_EXAMPLE
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mpl_toolkits.mplot3d <span style=color:#f92672>import</span> Axes3D
X_axis,Y_axis <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid([X<span style=color:#f92672>.</span>advertising<span style=color:#f92672>.</span>min(),X<span style=color:#f92672>.</span>advertising<span style=color:#f92672>.</span>max()],[X<span style=color:#f92672>.</span>competitors<span style=color:#f92672>.</span>min(),X<span style=color:#f92672>.</span>competitors<span style=color:#f92672>.</span>max()])

Z_axis <span style=color:#f92672>=</span> lr_model<span style=color:#f92672>.</span>params[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> lr_model<span style=color:#f92672>.</span>params[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> X_axis <span style=color:#f92672>+</span> lr_model<span style=color:#f92672>.</span>params[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>*</span> Y_axis
fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(figsize <span style=color:#f92672>=</span> (<span style=color:#ae81ff>12</span>,<span style=color:#ae81ff>8</span>))
ax <span style=color:#f92672>=</span> Axes3D(fig,azim<span style=color:#f92672>=-</span><span style=color:#ae81ff>100</span>)
ax<span style=color:#f92672>.</span>plot_surface(X_axis,Y_axis,Z_axis,alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>, linewidth<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)

ax<span style=color:#f92672>.</span>scatter(X<span style=color:#f92672>.</span>advertising,X<span style=color:#f92672>.</span>competitors,y)
</code></pre></div><p>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fc37c7f21c0>
<img src=/ox-hugo/r5IDzm.png alt></p></li></ul><ul><li><p>Classification practice</p><p>uses KNN algorithm:</p><ol><li>find similar points</li><li>select the most similar <code>k</code> points</li><li>the <code>mode</code> of label of the <code>k</code> points is the predicted label.</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>from</span> sklearn.neighbors <span style=color:#f92672>import</span> KNeighborsClassifier
<span style=color:#f92672>from</span> sklearn.model_selection  <span style=color:#f92672>import</span> train_test_split
df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;./resource/wine.csv&#34;</span>)
X_train,X_test,y_train,y_test <span style=color:#f92672>=</span> train_test_split(df[[<span style=color:#e6db74>&#34;density&#34;</span>,<span style=color:#e6db74>&#34;sulfates&#34;</span>,<span style=color:#e6db74>&#34;residual_sugar&#34;</span>]],df[<span style=color:#e6db74>&#34;high_quality&#34;</span>],test_size <span style=color:#f92672>=</span> <span style=color:#f92672>.</span><span style=color:#ae81ff>3</span>)
<span style=color:#75715e>#make classifers and fit</span>
classifier <span style=color:#f92672>=</span> KNeighborsClassifier(n_neighbors <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>)
classifier<span style=color:#f92672>.</span>fit(X_train,y_train)
<span style=color:#75715e>#test classifers</span>
prediction <span style=color:#f92672>=</span> classifier<span style=color:#f92672>.</span>predict(X_test)
<span style=color:#75715e>#calculate the correctness</span>
correct <span style=color:#f92672>=</span>  np<span style=color:#f92672>.</span>where(prediction <span style=color:#f92672>==</span> y_test,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>sum()
<span style=color:#66d9ef>print</span>(correct)

accuracy <span style=color:#f92672>=</span>  correct<span style=color:#f92672>/</span>len(y_test)
<span style=color:#66d9ef>print</span>(accuracy)

<span style=color:#75715e>#plot the accuracy from 1 to 51 step 2</span>
results<span style=color:#f92672>=</span> []
<span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>51</span>):
    classifier <span style=color:#f92672>=</span> KNeighborsClassifier(n_neighbors <span style=color:#f92672>=</span> k)
    classifier<span style=color:#f92672>.</span>fit(X_train,y_train)
    prediction <span style=color:#f92672>=</span> classifier<span style=color:#f92672>.</span>predict(X_test)
    accuracy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>where(prediction <span style=color:#f92672>==</span> y_test,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>sum()<span style=color:#f92672>/</span>(len(y_test))
    results<span style=color:#f92672>.</span>append([k,accuracy])

<span style=color:#75715e>#plotting</span>
results <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(results,columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;k&#34;</span>,<span style=color:#e6db74>&#34;accuracy&#34;</span>])
plt<span style=color:#f92672>.</span>plot(results<span style=color:#f92672>.</span>k,results<span style=color:#f92672>.</span>accuracy)
plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;k vs. accuracy&#34;</span>)
plt<span style=color:#f92672>.</span>show()

</code></pre></div><figure><img src=/ox-hugo/Q8E07V.png></figure></li></ul><ul><li><p>Cluster Practice</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>import</span> matplotlib.style <span style=color:#f92672>as</span> stl
stl<span style=color:#f92672>.</span>use(<span style=color:#e6db74>&#34;ggplot&#34;</span>)

<span style=color:#75715e>#get kmeans</span>
<span style=color:#f92672>from</span> sklearn.cluster <span style=color:#f92672>import</span> KMeans
X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>],[<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>],[<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>2</span>],[<span style=color:#ae81ff>11</span>,<span style=color:#ae81ff>4</span>],[<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>8</span>],[<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>9</span>],[<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>2</span>]])

kmeans <span style=color:#f92672>=</span> KMeans(n_clusters <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>)
kmeans<span style=color:#f92672>.</span>fit(X)

centroids <span style=color:#f92672>=</span> kmeans<span style=color:#f92672>.</span>cluster_centers_
labels <span style=color:#f92672>=</span> kmeans<span style=color:#f92672>.</span>labels_

centroids, labels

colors <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;g.&#34;</span>,<span style=color:#e6db74>&#34;r.&#34;</span>,<span style=color:#e6db74>&#34;c.&#34;</span>,<span style=color:#e6db74>&#34;y.&#34;</span>]

<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(X)):
    plt<span style=color:#f92672>.</span>plot(X[i][<span style=color:#ae81ff>0</span>],X[i][<span style=color:#ae81ff>1</span>],colors[labels[i]],markersize <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>)


plt<span style=color:#f92672>.</span>scatter(centroids[:,<span style=color:#ae81ff>0</span>],centroids[:,<span style=color:#ae81ff>1</span>], marker <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;x&#34;</span>,s <span style=color:#f92672>=</span> <span style=color:#ae81ff>150</span>,linewidth<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,zorder<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)

plt<span style=color:#f92672>.</span>show()
</code></pre></div><figure><img src=/ox-hugo/GqpjGC.png></figure></li></ul><ul><li><p>Density estimation</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>from</span> sklearn.cluster <span style=color:#f92672>import</span> MeanShift
<span style=color:#f92672>from</span> sklearn.datasets.samples_generator <span style=color:#f92672>import</span> make_blobs
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>from</span> mpl_toolkits.mplot3d <span style=color:#f92672>import</span> Axes3D
<span style=color:#f92672>import</span> matplotlib.style <span style=color:#f92672>as</span> stl
stl<span style=color:#f92672>.</span>use(<span style=color:#e6db74>&#34;ggplot&#34;</span>)
centers <span style=color:#f92672>=</span> [[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>],[<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>5</span>],[<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>10</span>]]
X,y <span style=color:#f92672>=</span> make_blobs(n_samples<span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>,centers<span style=color:#f92672>=</span> centers,cluster_std<span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>)
ms <span style=color:#f92672>=</span> MeanShift()
ms<span style=color:#f92672>.</span>fit(X)
centroids <span style=color:#f92672>=</span> ms<span style=color:#f92672>.</span>cluster_centers_
labels <span style=color:#f92672>=</span> ms<span style=color:#f92672>.</span>labels_
n_clusters_ <span style=color:#f92672>=</span> len(np<span style=color:#f92672>.</span>unique(labels))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;estimated clusters: &#34;</span> ,n_clusters_)

colors<span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;r&#34;</span>,<span style=color:#e6db74>&#34;g&#34;</span>,<span style=color:#e6db74>&#34;b&#34;</span>,<span style=color:#e6db74>&#34;c&#34;</span>,<span style=color:#e6db74>&#34;y&#34;</span>,<span style=color:#e6db74>&#34;m&#34;</span>,<span style=color:#e6db74>&#34;k&#34;</span>]

fig <span style=color:#f92672>=</span>  plt<span style=color:#f92672>.</span>figure()
ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>111</span>,projection <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;3d&#34;</span>)

<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(X)):
    ax<span style=color:#f92672>.</span>scatter(X[i][<span style=color:#ae81ff>0</span>],X[i][<span style=color:#ae81ff>1</span>],X[i][<span style=color:#ae81ff>2</span>],c <span style=color:#f92672>=</span> colors[labels[i]],marker <span style=color:#f92672>=</span><span style=color:#e6db74>&#34;o&#34;</span>)
ax<span style=color:#f92672>.</span>scatter(centroids[:,<span style=color:#ae81ff>0</span>],centroids[:,<span style=color:#ae81ff>1</span>],centroids[:,<span style=color:#ae81ff>2</span>], marker <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;x&#34;</span>, s <span style=color:#f92672>=</span> <span style=color:#ae81ff>150</span>, linewidth<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, zorder<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
plt<span style=color:#f92672>.</span>show()
</code></pre></div><figure><img src=/ox-hugo/HTCqHs.png></figure></li></ul><ul><li><p>Python</p><ul><li><p>Stat</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>164</span>, <span style=color:#ae81ff>158</span>, <span style=color:#ae81ff>172</span>, <span style=color:#ae81ff>153</span>, <span style=color:#ae81ff>144</span>, <span style=color:#ae81ff>156</span>, <span style=color:#ae81ff>189</span>, <span style=color:#ae81ff>163</span>, <span style=color:#ae81ff>134</span>, <span style=color:#ae81ff>159</span>, <span style=color:#ae81ff>143</span>, <span style=color:#ae81ff>176</span>, <span style=color:#ae81ff>177</span>, <span style=color:#ae81ff>162</span>, <span style=color:#ae81ff>141</span>, <span style=color:#ae81ff>151</span>, <span style=color:#ae81ff>182</span>, <span style=color:#ae81ff>185</span>, <span style=color:#ae81ff>171</span>, <span style=color:#ae81ff>152</span>])
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
plt<span style=color:#f92672>.</span>figure()
hist1,edges1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>histogram(X,bins<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
plt<span style=color:#f92672>.</span>bar(edges1[:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>],hist1,width <span style=color:#f92672>=</span> edges1[<span style=color:#ae81ff>1</span>:]<span style=color:#f92672>-</span>edges1[:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
plt<span style=color:#f92672>.</span>show()
<span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean(X),np<span style=color:#f92672>.</span>median(X),np<span style=color:#f92672>.</span>std(X)

</code></pre></div></li></ul><ul><li><p>Boston</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
<span style=color:#f92672>import</span> statsmodels.api <span style=color:#f92672>as</span> sm
df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;~/playground/General Resources/Data and code/Datasets/OA 5.7 - boston.csv&#34;</span>)
df<span style=color:#f92672>.</span>describe()

X <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;AGE&#34;</span>]
y <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;NOX&#34;</span>]
X <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>add_constant(X)

linear_model <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>OLS(y,X)<span style=color:#f92672>.</span>fit()
linear_model<span style=color:#f92672>.</span>rsquared
rsquares <span style=color:#f92672>=</span> []
<span style=color:#66d9ef>for</span> column <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns:
     X <span style=color:#f92672>=</span> df[column]
     y <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;NOX&#34;</span>]
     X <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>add_constant(X)
     linear_model <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>OLS(y,X)<span style=color:#f92672>.</span>fit()
     rsquares<span style=color:#f92672>.</span>append(linear_model<span style=color:#f92672>.</span>rsquared)
result <span style=color:#f92672>=</span> list(zip(df<span style=color:#f92672>.</span>columns,rsquares))
result
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[20]:
#+BEGIN_EXAMPLE
  [(&#39;CRIM&#39;, 0.17721718179269352),
  (&#39;ZN&#39;, 0.26687939094162105),
  (&#39;NDUS&#39;, 0.5831635323844071),
  (&#39;CHAS&#39;, 0.008317951975949422),
  (&#39;NOX&#39;, 1.0),
  (&#39;RM&#39;, 0.09131770087582114),
  (&#39;AGE&#39;, 0.535048512732641),
  (&#39;DIS&#39;, 0.5917149670934201),
  (&#39;RAD&#39;, 0.37385956267556064),
  (&#39;TAX&#39;, 0.44625499627669685),
  (&#39;PTRATIO&#39;, 0.035695556480997426),
  (&#39;B&#39;, 0.1444384872864103),
  (&#39;LSTAT&#39;, 0.3491378991413132),
  (&#39;MEDV&#39;, 0.1826030425016989)]
#+END_EXAMPLE
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>X <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>DIS
y <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>NOX

X <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>add_constant(X)
X_axis <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(X<span style=color:#f92672>.</span>DIS<span style=color:#f92672>.</span>min(),X<span style=color:#f92672>.</span>DIS<span style=color:#f92672>.</span>max(),<span style=color:#ae81ff>100</span>)
X_axis <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>add_constant(X_axis)
linear_model <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>OLS(y,X)<span style=color:#f92672>.</span>fit()
y_hat <span style=color:#f92672>=</span> linear_model<span style=color:#f92672>.</span>predict(X_axis)
plt<span style=color:#f92672>.</span>figure()
plt<span style=color:#f92672>.</span>plot(X_axis,y_hat)
plt<span style=color:#f92672>.</span>show()
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;the equation is :&#34;</span>, <span style=color:#e6db74>&#34;y = &#34;</span>, linear_model<span style=color:#f92672>.</span>params[<span style=color:#ae81ff>0</span>], <span style=color:#e6db74>&#34; + &#34;</span>, linear_model<span style=color:#f92672>.</span>params[<span style=color:#ae81ff>1</span>], <span style=color:#e6db74>&#34; * x&#34;</span> )
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[36]:
[[file:./obipy-resources/Oywtol.png]]
</code></pre></div><p><img src=/ox-hugo/cNPPVS.png alt>
<img src=/ox-hugo/w9tl7I.png alt></p></li></ul><ul><li><p>IRIS</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
<span style=color:#f92672>import</span> matplotlib.style <span style=color:#f92672>as</span> stl
stl<span style=color:#f92672>.</span>use(<span style=color:#e6db74>&#34;ggplot&#34;</span>)
<span style=color:#f92672>from</span> sklearn.neighbors <span style=color:#f92672>import</span> KNeighborsClassifier
<span style=color:#f92672>from</span> sklearn.svm <span style=color:#f92672>import</span> SVC
<span style=color:#f92672>from</span> sklearn <span style=color:#f92672>import</span> datasets
<span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split

iris <span style=color:#f92672>=</span> datasets<span style=color:#f92672>.</span>load_iris()
df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(iris<span style=color:#f92672>.</span>data)
df[<span style=color:#e6db74>&#34;class&#34;</span>] <span style=color:#f92672>=</span> iris<span style=color:#f92672>.</span>target
df<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Sepal length&#34;</span>,<span style=color:#e6db74>&#34;Sepal width&#34;</span>,<span style=color:#e6db74>&#34;Pedal langth&#34;</span>,<span style=color:#e6db74>&#34;Pedal width&#34;</span>,<span style=color:#e6db74>&#34;Class&#34;</span>]

X_train,X_test,y_train,y_test <span style=color:#f92672>=</span> train_test_split(df[[<span style=color:#e6db74>&#34;Sepal length&#34;</span>,<span style=color:#e6db74>&#34;Sepal width&#34;</span>]],df[<span style=color:#e6db74>&#34;Class&#34;</span>],test_size <span style=color:#f92672>=</span> <span style=color:#f92672>.</span><span style=color:#ae81ff>3</span>)

classifier <span style=color:#f92672>=</span> KNeighborsClassifier(n_neighbors <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>)
classifier<span style=color:#f92672>.</span>fit(X_train,y_train)

prediction <span style=color:#f92672>=</span> classifier<span style=color:#f92672>.</span>predict(X_test)
correct <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>where(prediction <span style=color:#f92672>==</span> y_test, <span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>sum()

accuracy <span style=color:#f92672>=</span> correct<span style=color:#f92672>/</span>(len(y_test))
accuracy




<span style=color:#75715e>#kmeans = KNeighborsClassifier(n_neighbors = 4)</span>
<span style=color:#75715e>#kmeans.fit()</span>
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[19]:
: 0.7111111111111111
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
svm_kernel_types <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;linear&#34;</span>,<span style=color:#e6db74>&#34;rbf&#34;</span>,<span style=color:#e6db74>&#34;poly&#34;</span>]
svm <span style=color:#f92672>=</span> SVC(kernel <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;poly&#34;</span>)
svm<span style=color:#f92672>.</span>fit(X_train,y_train)

prediction <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>predict(X_test)
correct <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>where(prediction <span style=color:#f92672>==</span> y_test,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>sum()
accuracy <span style=color:#f92672>=</span> correct<span style=color:#f92672>/</span>(len(y_test))
accuracy

</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[25]:
: 0.7555555555555555
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> sklearn.cluster <span style=color:#f92672>import</span> KMeans
kmeans <span style=color:#f92672>=</span> KMeans(n_clusters <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>)
kmeans<span style=color:#f92672>.</span>fit(df[[<span style=color:#e6db74>&#34;Sepal length&#34;</span>,<span style=color:#e6db74>&#34;Sepal width&#34;</span>]])

centroids,labels <span style=color:#f92672>=</span> kmeans<span style=color:#f92672>.</span>cluster_centers_,kmeans<span style=color:#f92672>.</span>labels_
colors <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;g&#34;</span>,<span style=color:#e6db74>&#34;r&#34;</span>,<span style=color:#e6db74>&#34;c&#34;</span>,<span style=color:#e6db74>&#34;y.&#34;</span>]

<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(df)):
     plt<span style=color:#f92672>.</span>scatter(df[<span style=color:#e6db74>&#34;Sepal length&#34;</span>][i],df[<span style=color:#e6db74>&#34;Sepal width&#34;</span>][i],c <span style=color:#f92672>=</span> colors[labels[i]])

plt<span style=color:#f92672>.</span>scatter(centroids[:,<span style=color:#ae81ff>0</span>],centroids[:,<span style=color:#ae81ff>1</span>],marker <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;x&#34;</span>,s <span style=color:#f92672>=</span> <span style=color:#ae81ff>150</span>, linewidth <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>, zorder <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>)
df[<span style=color:#e6db74>&#34;Sepal length&#34;</span>][<span style=color:#ae81ff>2</span>]
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[44]:
: 4.7
</code></pre></div><figure><img src=/ox-hugo/MEzL5y.png></figure></li></ul><ul><li><p>Breast Cancer</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>from</span> sklearn.cluster <span style=color:#f92672>import</span> KMeans
<span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;/Users/hermanhe/playground/General Resources/Data and code/Datasets/dataR2.csv&#34;</span>)
type(df[<span style=color:#e6db74>&#34;Leptin&#34;</span>][<span style=color:#ae81ff>1</span>])
df[<span style=color:#e6db74>&#34;Leptin&#34;</span>] <span style=color:#f92672>=</span> list(map(<span style=color:#66d9ef>lambda</span> x:int(x <span style=color:#f92672>*</span> <span style=color:#ae81ff>100</span>)<span style=color:#f92672>/</span><span style=color:#ae81ff>100</span>,df[<span style=color:#e6db74>&#34;Leptin&#34;</span>]))
X <span style=color:#f92672>=</span> df[df<span style=color:#f92672>.</span>columns[:<span style=color:#ae81ff>9</span>]]
kmeans <span style=color:#f92672>=</span> KMeans(n_clusters <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>)
kmeans<span style=color:#f92672>.</span>fit(X)
labels <span style=color:#f92672>=</span> kmeans<span style=color:#f92672>.</span>labels_

accuracy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>where(labels<span style=color:#f92672>!=</span>df<span style=color:#f92672>.</span>Classification,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>sum()<span style=color:#f92672>/</span>(len(labels))
accuracy



</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text># Out[41]:
: 0.896551724137931
</code></pre></div></li></ul></li></ul></li></ul></li></ul><ul><li><p>TODO Part III: Machine Learning for Data Science</p><ul><li>TODO Going through the key terms with <a href=/braindump/main/20211011020236-feynman_techinique/>Feynman Techinique</a></li></ul><ul><li>TODO Answer the Conceptual questions</li></ul><ul><li>TODO Do the Hands=on Problems</li></ul></li></ul><ul><li><p>TODO Part IV: Application, evaluations, and methods.</p><ul><li>TODO Going through the key terms with <a href=/braindump/main/20211011020236-feynman_techinique/>Feynman Techinique</a></li></ul><ul><li>TODO Answer the Conceptual questions</li></ul><ul><li>TODO Do the Hands=on Problems</li></ul></li></ul></li></ul></li></ul><ul><li><p>THREAD Project Euler agenda</p><ul><li><p>Scheduling Recommandation</p><ul><li><p>Time</p><p>none
could frequently.</p></li></ul><ul><li><p>Recource</p><p><a href=https://projecteuler.net>Project Euler</a></p></li></ul></li></ul><ul><li><p>DONE Prob. 9</p><p><a href=~/playground/clojure/.#project_euler.clj>code</a>
use algebra.
a = 2mn, b = m2-n2, c = m2+ n2.
==> m>n>0, m(m+n)=500. 500 = 2 * 2 * 5 * 5 * 5</p><table><thead><tr><th>m</th><th>m+n</th><th>correct?</th></tr></thead><tbody><tr><td>2</td><td>250</td><td>no</td></tr><tr><td>4</td><td>125</td><td>no</td></tr><tr><td>20</td><td>25</td><td>yes</td></tr><tr><td>100</td><td>5</td><td>no</td></tr><tr><td>500</td><td>1</td><td>no</td></tr><tr><td>10</td><td>50</td><td>no</td></tr></tbody></table></li></ul><ul><li><p>TODO Prob. 10</p><p>sum of primes below 2,000,000</p></li></ul><ul><li><p>Prob.13</p><p>The brute force. And a slightly performance version.
Both use the long-add function.</p></li></ul></li></ul><h3 id=coding>Coding</h3><h4 id=habit-contract>Habit contract</h4><p>I like coding, and I want be better at coding and engineering, I want to be an exellent hacker, so I code everyday, with conscious and wise mind.</p><p>I shall code with my wits, my integrity, and my dedicated loyalty.
I shall not code with distracting, nor code dumb without thinking
I shall log everything I see and thought about them properly.
&ndash;> I shall code to the standard of <a href=/braindump/main/20220326155217-coding_machine/>coding machine</a></p><p>And if I haven&rsquo;t achieve all the descriptions above, I shall add 2 more minutes of running/10 more pull up this day. Record in the day&rsquo;s page.</p><ul><li><ul><li>coding(to a book/a problem I encounter)</li></ul><ul><li><p>Cue - obvious</p><ul><li>Habit stack: I&rsquo;ll start coding, Right after I take my morning dump. I could use toilet time to think/deside what to do</li><li>environment: I ahve ACP side of my desk.</li></ul></li></ul><ul><li><p>Craving - attractive</p><ul><li>Reprogramming the brain<ul><li>the benefit of improving my coding skill:<ol><li>I&rsquo;ll be more capable in engineering.</li><li>I&rsquo;ll be powerful</li><li>People would admire me</li><li>I&rsquo;ll have a lot of fun in programming and engineering a system</li><li>It&rsquo;ll enable me to eventually build my baby AI</li><li>The whole thing of coding/engineering would be more clear to me, instead of vague and frustrating mistery</li></ol></li></ul></li><li>Temptation bundling
After I&rsquo;ve done a session of coding, I&rsquo;ll practice a knot</li></ul></li></ul><ul><li><p>Response - easy</p><ul><li>least effort:
I just have to code for 1 session to whatever material at hand.
It could be from the <em>Hands-on intro to ML</em> book, or a projectEuler challenge, or a random program from my todo list, anything would be suffice</li><li>Prime the environment:
Choose a coding task before I went to toilet, and open the file I needed.</li><li>Use a 2-minuet version:
Do 2-minute coding first if procrastinate. But I doubt if I will.</li></ul></li></ul><ul><li><p>Reward - satisfying</p><ul><li>Adding little pleasure
After the session, tie a knot.
They are both engineering & art</li><li>Habit track
Make a subtree coding in Engineering, style habit. Each day ofter the 1 session, set it marked.</li><li>Habit contract
In the heading, make declaims</li><li>Visual measurements
log every thing done. Round thing up</li></ul></li></ul><ul><li><p>Mindset - continue process</p><ul><li>I&rsquo;m a hacker</li><li>1% improvement a day.</li><li>showing up&rsquo;s half the bottle</li></ul></li></ul></li></ul><h4 id=logs>logs</h4><ul><li><p>2022</p><ul><li><p>2022-03 March</p><ul><li><p>2022-03-26 Saturday</p><ul><li>11:10 AM - coding on org-capture in chrome</li></ul></li></ul><ul><li><p>2022-03-27 Sunday</p><ul><li><p>10:52 AM - coding on ML</p><p><a href=#classification-practice>Classification practice</a></p><p>doing KNN of classification practice with <em>Hands-on Intro of ML</em>
Then applied it to the CW_Data.csv data</p></li></ul><ul><li><p>02:18 PM - coding on ML-cluster</p><p><a href=/braindump/main/20220224134643-int104/#cluster-use-q1-and-q2>Cluster: use Q1 and Q2</a></p><p>sklearn.Cluster.KMeans attributes_, cluster_centers_, labels_.</p><p>use <code>zip</code> to zip 2 columns (with pandas, very handy)</p><p>Apply the technique to CW2 Q1 and Q2, very neat clustering. Very different with the real cluster with label programme, which is interleaved.</p></li></ul><ul><li><p>02:51 PM - coding on density estimation</p><p>Doing a dencity estimation in <em>Hands-on</em>
have the results not in a drawer, but prefixed by <code>:</code> can have the history not edited.(if I hand remove the <code>:</code>)
When with CW_Data, 1 looks similar.</p></li></ul><ul><li><p>04:34 PM - coding on python problems</p><p>going through the problems in <em>Hands-on</em></p></li></ul></li></ul></li></ul><ul><li><p>2022-04 April</p><ul><li>2022-04-12 Tuesday</li></ul></li></ul></li></ul></div><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=/braindump/daily/2021-11-04/>2021-11-04</a></li><li><a href=/braindump/daily/2022-03-26/>2022-03-26</a></li><li><a href=/braindump/main/20211114195701-me_meta/>Me_meta</a></li><li><a href=/braindump/main/20211008132927-roles/>Roles</a></li></ul></div></div></div></div></div><script src=https://hermanhel.github.io/braindump/js/URI.js type=text/javascript></script><script src=https://hermanhel.github.io/braindump/js/page.js type=text/javascript></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>